{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_NER Exp2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw-ll7pwF4QU"
      },
      "source": [
        "### Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7-piwdovGck",
        "outputId": "1a37281e-0b62-4c51-c6ff-0401680f6c93"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download(\"words\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/kiran/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /home/kiran/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/kiran/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /home/kiran/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /home/kiran/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-YbNiUcF-8n"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyrY5HrhtznB",
        "outputId": "2a79c6c1-7632-4628-f6c3-b17fa570ad93"
      },
      "source": [
        "txt = \"Heavy rain continues to lash parts of western Maharashtra, Raigad and Thane districts with more than 60 barrages submerged in Kolhapur district\"\n",
        "tokenized = sent_tokenize(txt)\n",
        "print(tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Heavy rain continues to lash parts of western Maharashtra, Raigad and Thane districts with more than 60 barrages submerged in Kolhapur district']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0cYLOfzGC6t"
      },
      "source": [
        "### String Tokenization And Parts of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WlrhAWcvQH5",
        "outputId": "4ab0dfae-e694-4666-ee30-1e9be3dcf511"
      },
      "source": [
        "# Passing sentence by sentence in the for loop\n",
        "for i in tokenized:\n",
        "    # Tokenizing the sentence\n",
        "    wordsList = nltk.word_tokenize(i)\n",
        "    # Removing stop words like is, on , and, ...\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        "    # Task1 Call the method that does Parts of Speech Tagging from nltk library and the pass the wordsList\n",
        "    # tagged = ...\n",
        "    # Your code here\n",
        "    \n",
        "    print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Heavy', 'NNP'), ('rain', 'NN'), ('continues', 'VBZ'), ('lash', 'JJ'), ('parts', 'NNS'), ('western', 'JJ'), ('Maharashtra', 'NNP'), (',', ','), ('Raigad', 'NNP'), ('Thane', 'NNP'), ('districts', 'NNS'), ('60', 'CD'), ('barrages', 'NNS'), ('submerged', 'VBN'), ('Kolhapur', 'NNP'), ('district', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWanDr3YDCCC"
      },
      "source": [
        "Representations:\n",
        "\n",
        "CC \tcoordinating conjunction\n",
        "CD \tcardinal digit\n",
        "DT \tdeterminer\n",
        "EX \texistential there\n",
        "FW \tforeign word\n",
        "IN \tpreposition/subordinating conjunction\n",
        "JJ \tThis NLTK POS Tag is an adjective (large)\n",
        "JJR \tadjective, comparative (larger)\n",
        "JJS \tadjective, superlative (largest)\n",
        "LS \tlist market\n",
        "MD \tmodal (could, will)\n",
        "NN \tnoun, singular (cat, tree)\n",
        "NNS \tnoun plural (desks)\n",
        "NNP \tproper noun, singular (sarah)\n",
        "NNPS \tproper noun, plural (indians or americans)\n",
        "PDT \tpredeterminer (all, both, half)\n",
        "POS \tpossessive ending (parent\\ 's)\n",
        "PRP \tpersonal pronoun (hers, herself, him,himself)\n",
        "PRP$ \tpossessive pronoun (her, his, mine, my, our )\n",
        "RB \tadverb (occasionally, swiftly)\n",
        "RBR \tadverb, comparative (greater)\n",
        "RBS \tadverb, superlative (biggest)\n",
        "RP \tparticle (about)\n",
        "TO \tinfinite marker (to)\n",
        "UH \tinterjection (goodbye)\n",
        "VB \tverb (ask)\n",
        "VBG \tverb gerund (judging)\n",
        "VBD \tverb past tense (pleaded)\n",
        "VBN \tverb past participle (reunified)\n",
        "VBP \tverb, present tense not 3rd person singular(wrap)\n",
        "VBZ \tverb, present tense with 3rd person singular (bases)\n",
        "WDT \twh-determiner (that, what)\n",
        "WP \twh- pronoun (who)\n",
        "WRB \twh- adverb (how) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA5YD9sN5F43"
      },
      "source": [
        "\n",
        "### Named Entity Recognition\n",
        "\n",
        "**Named Entity Recognition** (NER) is a standard NLP problem which involves spotting named entities (people, places, organizations etc.) from a chunk of text, and classifying them into a predefined set of categories.\n",
        "\n",
        "Refer: https://www.nltk.org/book/ch07.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNJBylHZ5KUe",
        "outputId": "9539c6e4-74ac-4db5-cd29-4c42515b0a95"
      },
      "source": [
        "# Sample sentence\n",
        "\n",
        "sentence = \"Heavy rain continues to lash parts of western Maharashtra districts with more than 60 barrages submerged in Kolhapur district.\"\n",
        "\n",
        "# Sending sentence by sentence inside the loop\n",
        "for sent in nltk.sent_tokenize(sentence):\n",
        "    # We will form chunks based on the tokenized words and pos tagging\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "        # The chunks will hold a attribute label if it has identified that the chunk is a Named Entity\n",
        "        if hasattr(chunk, 'label'):\n",
        "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPE Heavy\n",
            "GPE Kolhapur\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo_gK9imDCCD"
      },
      "source": [
        "***NE Type \tExamples***\n",
        "ORGANIZATION \tGeorgia-Pacific Corp., \n",
        "PERSON \tEddy Bonte, President Obama\n",
        "LOCATION \tMurray River, Mount Everest\n",
        "DATE \tJune, 2008-06-29\n",
        "TIME \ttwo fifty a m, 1:30 p.m.\n",
        "MONEY \t175 million Canadian Dollars, \n",
        "PERCENT \ttwenty pct, 18.75 %\n",
        "FACILITY \tWashington Monument, Stonehenge\n",
        "GPE \tSouth East Asia, Midlothian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFRWYNpZ5ZGN"
      },
      "source": [
        "# Task 2 \n",
        "# List a few named entities that you can think of.\n",
        "# Check if those entities are being caputered by nltk library by passing a sentence containing that named entitiy in the above code."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC6KKDKeDCCD"
      },
      "source": [
        "# Example for other Parsers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxzE37PnDCCD",
        "outputId": "9eb23e48-5d45-49cf-d18c-8eeedcee75b4"
      },
      "source": [
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "\n",
        "# Download the parser from https://nlp.stanford.edu/software/stanford-parser-4.2.0.zip\n",
        "# Extract the folder and link stanford-parser.jar\n",
        "path_jar = \"/home/kiran/ta/nlp/nlp_assignment/Word2vec_MB/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser.jar\"\n",
        "# Link stanford-parser-4.2.0-models.jar\n",
        "path_models_jar = \"/home/kiran/ta/nlp/nlp_assignment/Word2vec_MB/stanford-parser-4.2.0/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models.jar\"\n",
        "\n",
        "dep_parser = StanfordDependencyParser(path_to_jar = path_jar, path_to_models_jar = path_models_jar)\n",
        "\n",
        "result = dep_parser.raw_parse(\"I saw an elephant in my sleep\")\n",
        "dependency = result.__next__()\n",
        "\n",
        "#Print the results of the parser\n",
        "print(list(dependency.triples()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('saw', 'VBD'), 'nsubj', ('I', 'PRP')),\n",
              " (('saw', 'VBD'), 'obj', ('elephant', 'NN')),\n",
              " (('elephant', 'NN'), 'det', ('an', 'DT')),\n",
              " (('saw', 'VBD'), 'obl', ('sleep', 'NN')),\n",
              " (('sleep', 'NN'), 'case', ('in', 'IN')),\n",
              " (('sleep', 'NN'), 'nmod:poss', ('my', 'PRP$'))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    }
  ]
}