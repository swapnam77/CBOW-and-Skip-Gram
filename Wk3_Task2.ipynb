{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wk3 Task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taMIEZrmvbIe",
        "outputId": "49b94af4-48a1-4140-b180-d50cfd838257"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#Experiment 2-Task 1: Tag all the parts of speech for the given sentences using nltk library or library of your choice.\n",
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import brown\n",
        "from gensim.models import Word2Vec\n",
        "import multiprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "KN5MHhxXvjXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=brown.sents()\n",
        "print(sentences[:3])"
      ],
      "metadata": {
        "id": "_M-OnxEwvm2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM = 300\n",
        "\n",
        "w2v = Word2Vec(sentences, size=EMB_DIM, window=5, min_count=5, negative=15,\n",
        "                iter=10, workers=multiprocessing.cpu_count())"
      ],
      "metadata": {
        "id": "xLVPtrqsvxTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors=w2v.wv #get trained embeddings - an KeyedVectors instance\n",
        "result=word_vectors.similar_by_word(\"Knowledge\")\n",
        "print(\"Most similar to 'Knowledge', result[:3])\n",
        "\n",
        "\n",
        "result=word_vectors.similar_by_word(\"Above\")\n",
        "print(\"Most similar to 'Above', result[:3])\n",
        "\n",
        "\n",
        "result=word_vectors.similar_by_word(\"sakuni\")\n",
        "print(\"Most similar to 'sakuni', result[:3])"
      ],
      "metadata": {
        "id": "9jemTu80wDzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment 1-Task 2: Train a word2vec skip gram model with gensim package and\n",
        "# compare the performance with cbow model based on known similarity between words.\n",
        "from nltk.corpus import conl1200\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.Keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "# getting tokenised and part-of-speech tagged data from the corpus\n",
        "train_words = conl12000.tagged_words(\"train.txt\")\n",
        "test_words = conl12000.tagged_words(\"test.txt\")\n",
        "print(train_words[:10])"
      ],
      "metadata": {
        "id": "E-Np4llgwMlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tag_vocabulary(tagged_words)\n",
        "\"\"\"\n",
        "Accepts text in the form of (word,pos) tuples and returns a dictionary\n",
        "mapping POS-tags to unique ids.\n",
        "\"\"\"\n",
        "tag2id = {}\n",
        "for item in tagged_words:\n",
        "tag = item[1]\n",
        "tag2id.setdefault(tag, len(tag2id))\n",
        "return tag2id\n",
        "\n",
        "#the word_vectors.vocab dictionary stores Vocab objects, rather than integers\n",
        "# but we would like our dictionary to map words to ints\n",
        "word2id = {k:v.index for k, v in word_vectors.vocab.items()}\n",
        "tag2id = get_tag_vocabulary(train_words)\n"
      ],
      "metadata": {
        "id": "LxvBdp_VwOep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_int_data(tagged_words, word2id, tag2id):\n",
        "\n",
        "X,Y =[], []\n",
        "unk_count =0\n",
        "\n",
        "for word,tag in tagged_words:\n",
        "Y.append(tag2id.get(tag))\n",
        "if word in word2id:\n",
        "X.append(word2id.get(word))\n",
        "else:\n",
        "unk_count += 1\n",
        "print(\"Data created. Percentage of unknown words: %.3f\" % (unk_count/len(tagged_words)))\n",
        "return np.array(X), np.array(Y)\n",
        "\n",
        "X_train, Y_train = get_int_data(train_words,word2id, tag2id)\n",
        "X_trest, Y_test = get_int_data(test_words,word2id, tag2id)\n",
        "\n",
        "\n",
        "# we need to one-hot encode the tag indexes\n",
        "Y_train, Y_test =to_categorical(Y_train), to_categorical(Y_test)"
      ],
      "metadata": {
        "id": "x3r0rAUiwtu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def add_new_word(new_word, new_vector, new_index, embedding_matrix,word2id):\n",
        "\"\"\"\n",
        "Adds a new word to the existing matrix of word embeddings.\n",
        "\"\"\"\n",
        "\n",
        "#inserting the vector before given index, along axis 0\n",
        "embedding_matrix= np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
        "\n",
        "#updating the indexes of words that follow the new word\n",
        "word2id= {word: (index+1) if index >= new_index else index\n",
        "           for word, index in word2id.items()}\n",
        "word2id[new_word] = new_index\n",
        "return embedding_matrix, word2id\n",
        "\n",
        "UNK_INDEX=0 # it is generally common to associate UNK with index 0\n",
        "UNK_TOKEN=\"UNK\"\n",
        "\n",
        "embedding_matrix = word_vectors.vectors\n",
        "unk_vector = embedding_matrix.mean(0)\n",
        "embedding_matrix, word2id= add_new_word(UNK_TOKEN, unk_vector,\n",
        "                                         UNK_INDEX, embedding_matrix, word2id)"
      ],
      "metadata": {
        "id": "9zZrurZDw4bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_int_data(tagged_words, word2id, tag2id):\n",
        "\"\"\"\n",
        "Replaces all words and tags with their corresponding ids and\n",
        "seperates words(features) from the tags(labels).\n",
        "\"\"\"\n",
        "\n",
        "X, Y = [], []\n",
        "unk_count = 0\n",
        "\n",
        "for word, tag in tagged_words:\n",
        "  Y.append(tag2id.get(tag))\n",
        "  if word in word2id:\n",
        "      X.append(UNK_INDEX)\n",
        "      unk_count += 1\n",
        "   print(\"Data created. Percentage of unknown words: %.3f\" %(unk_count/len(tagged_words)))\n",
        "   return np.array(X), np.array(Y)\n",
        "\n",
        "X_train, Y_train = get_int_data(train_words, word2id,tag2id)\n",
        "X_test, Y_test = get_int_data(test_words, word2id, tag2id)\n",
        "Y_train, Y_test =to_categorical(Y_train), to_categorical(Y_test)\n"
      ],
      "metadata": {
        "id": "7KkCykDnxCC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def define_model(embedding_matrix, class_count):\n",
        "\"\"\"\n",
        "   Creates and returns a simple part-of-speech model, which\n",
        "   takes only one word as input.\n",
        "   \"\"\"\n",
        " vocab_length = len(embedding_matrix)\n",
        " model= Sequential()\n",
        "\n",
        "# A layer which turns word indexes into vectors\n",
        "model.add(Embedding(input_dim=vocab_length,\n",
        "                    output_dim=EMB_DIM,\n",
        "                    weights=[embedding_matrix],\n",
        "\t             input_length=1))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(HIDDEN_SIZE))\n",
        "model.add(Activation(\"tanh\"))\n",
        "model.add(Dense(class_count))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "               loss=\"categorical_crossentropy\",\n",
        "               metrics=[\"accuracy\"])\n",
        "return model\n",
        "\n",
        "pos_model = define_model(embedding_matrix, len(tag2id))\n",
        "pos_model.summary()\n",
        "\n",
        "#Training the model\n",
        "pos_model.fit(X_train,\n",
        "              Y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=1,\n",
        "              verbose=1)"
      ],
      "metadata": {
        "id": "AVylg7M2xL3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment 2-Task 2: List the named entities that you want a model to predict and \n",
        "#check if the model predicts those named entities for a sample sentence.\n",
        "def eva1uate_model(model, id2word, x_test, y_test):\n",
        "\"\"\"\n",
        "Evaluates the given model by computing the accuracy of its predictions\n",
        "on the given test data and prints out 10 most mistagged words.\n",
        "\"\"\"\n",
        "\n",
        "_, acc = model.evaluate(x_test, y_test)\n",
        "print(\"Accuracy: %.2f\" %acc)\n",
        "\n",
        "y_pred = model.predict_classes(x_test)\n",
        "error_counter = collections.Counter()\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "   correct_tag_id = np.argmax(y_test[i])\n",
        "   if y_pred[i]! = correct_tag_id:\n",
        "      word = id2word[x_test[i]]\n",
        "      error_counter[word] +=1\n",
        "   print(\"Most common errors:\\n\", error_counter.most_common(10))\n",
        "\n",
        "id2word = sorted(word2id, key=wordid.get)\n",
        "evaluate_model(pos_model, id2word, X_test, Y_test)\n"
      ],
      "metadata": {
        "id": "RNYiT8PTxbMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_INDEX = 1\n",
        "EOS_TOKEN = \"EOS\"\n",
        "\n",
        "#creating a random end-of-sequence vector\n",
        "eos_vector = np.random.standard_normal(EMB_DIM)\n",
        "embedding_matrix, word2id = add_new_word(EOS_TOKEN, eos_vector, EOS_INDEX,embedding_matrix, word2id)\n",
        "\n"
      ],
      "metadata": {
        "id": "vmIRHynXxr7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2 #define the size of the context-window\n",
        "\n",
        "def get_window_int_data(tagged_words, word2id, tag2id):\n",
        "\n",
        "X,Y = [], []\n",
        "unk_count = 0\n",
        "\n",
        "span= 2* CONTEXT_SIZE + 1\n",
        "buffer = collections.deque(maxlen=span)\n",
        "padding = [(EOS_TOKEN, None)] * CONTEXT_SIZE\n",
        "buffer +=padding + tagged_words[:CONTEXT_SIZE]\n",
        "\n",
        "for item in (tagged_words[context_size:] + padding):\n",
        "  buffer.append(item)\n",
        "\n",
        "# the input to the model is the ids of all words in the window\n",
        "window_ids = np.array([word2id.get(word) if (word in word2id) else UNK_INDEX\n",
        "                       for (word, _) in buffer])\n",
        "X.append(window_ids)\n",
        "\n",
        "#the label is the tag of the middle word\n",
        "middle_word, middle_tag = buffer[CONTEXT_SIZE]\n",
        "Y.append(tag2id.get(middle_tag))\n",
        "\n",
        "if middle_word not in word2id:\n",
        "unk_count +=1\n",
        "print(\"Data created. Percentage of unknown words: %.3f\" %(unk_count/len(tagged_words)))\n",
        "return np.array(X), np.array(Y)\n"
      ],
      "metadata": {
        "id": "PNHd740ix4Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_context_sensitive_model(embedding_matrix, class_count):\n",
        "\"\"\"\n",
        "Creates and returns a part-of-speech model, which\n",
        "takes as input a tagged word and its context.\n",
        "\"\"\"\n",
        "vocab_length = len(embedding_matrix)\n",
        "total_span = CONTEXT_SIZE*2 +1\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_length, output_dim=EMB_DIM, weights=[embedding_matrix],\n",
        "                    input_length=total_span)),\n",
        "model.add(Flatten())\n",
        "model.add(Dense(HIDDEN_SIZE))\n",
        "model.add(Activation(\"tanh\"))\n",
        "model.add(Dense(class_count))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "               loss=\"categorical_crossentropy\",\n",
        "               metrics=[\"accuracy\"])\n",
        "return model\n",
        "\n",
        "def evaluate_model(model, id2word, x_test, y_test):\n",
        "\"\"\"\n",
        "Evaluates the given model by computing the accuracy of its predictions\n",
        "on the given test data and prints out 10 most mistagged words.\n",
        "\"\"\"\n",
        "\n",
        "_, acc = model.evaluate(x_test, y_test)  #get accuracy of the model\n",
        "print(\"Accuracy: %.2f\" % acc)\n",
        "\n",
        "#the following lines are used to get most commonly mistagged words\n",
        "y_pred= model.predict_classes(x_test) #get model predictions\n",
        "error_counter = collections.Counter() #we will use a counter instance\n",
        "\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "  correct_tag_id = np.argmax(y_test[i])\n",
        "  if y_new[i]!= correct_tag_id:\n",
        "  if isinstance(x_test[i], np.ndarray):\n",
        "      word = id2word[x_test[i][CONTEXT_SIZE]]\n",
        "      else:\n",
        "      word= id2word[x_test[i]]\n",
        "     error_counter[word] += 1\n",
        "print(\"Most common errors:\\n\" , error_counter.most_common(10))\n"
      ],
      "metadata": {
        "id": "-tmDz23Lx_r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2, Y_train2 = get_window_int_data(train_words, word2id, tag2id)\n",
        "X_test2, Y_test2= get_window_int_data(test_words, word2id, tag2id)\n",
        "Y_train2, Y_test2 = to_categorical(Y_train2), to_categorical(Y_test2)\n",
        "\n",
        "cs_pos_model = define_context_sensitive_model(embedding_matrix, len(tag2id))\n",
        "cs_pos_model.fit(X_train2, Y_train2, batch_size =BATCH_SIZE, epochs=1, verbose=1)\n",
        "\n",
        "evaluate_model(cs_pos_model, id2word, X_test2, Y_test2) "
      ],
      "metadata": {
        "id": "krt0oKauycU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}